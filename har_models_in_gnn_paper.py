# -*- coding: utf-8 -*-
"""HAR models in GNN paper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y8dKyaHy4vmLl33-e8QqVKxVH81e9EYv
"""

import pandas as pd
import numpy as np
from scipy.stats import norm
import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import accuracy_score, roc_auc_score
import lightgbm as lgb
import xgboost as xgb
from lightgbm import early_stopping, log_evaluation
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")
import os
base = os.path.dirname(__file__)
import math
import time
import joblib
import random
from copy import deepcopy
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# function to compute daily BPV
def compute_bpv(df):
    # μ1 constant
    mu1 = np.sqrt(2/np.pi)
    bpv_list = []
    grouped = df.groupby("Date")  # group by day

    for date, group in grouped:
        r = group["log_return"].values  # intraday returns
        if len(r) < 2:
            bpv_list.append({"Date": date, "BPV": np.nan})
            continue
        bpv = np.sum(np.abs(r[1:]) * np.abs(r[:-1])) / (mu1**2)
        bpv_list.append({"Date": date, "BPV": bpv})

    return pd.DataFrame(bpv_list)

# Define functions for RV+ and RV-
def rv_plus(x):
    return np.sum(np.square(x[x > 0]))

def rv_minus(x):
    return np.sum(np.square(x[x < 0]))

def realized_tri_power_quarticity(group):
    r = group["log_return"].values
    M = len(r)
    if M < 3:
        return np.nan
    tq = (M / (M - 2)) * np.sum(np.abs(r[2:])**(4/3) * np.abs(r[1:-1])**(4/3) * np.abs(r[:-2])**(4/3))
    tq *= mu_4by3 ** (-3)
    return tq

def data(list1):
  X_train = train_df[list1]
  y_train = train_df[['RV_dir']]
  X_test = test_df[list1]
  y_test = test_df[['RV_dir']]
  X_train = sm.add_constant(X_train)
  X_test = sm.add_constant(X_test)
  return X_train, y_train, X_test, y_test

def perf_eval(y_pred,y_test):
  from sklearn.metrics import accuracy_score
  # Convert predicted probabilities to binary 0/1 using threshold = 0.5
  y_pred_class = (y_pred >= 0.5).astype(int)

  accuracy = round(accuracy_score(y_test, y_pred_class),4)

  from sklearn.metrics import roc_auc_score
  auc = round(roc_auc_score(y_test, y_pred),4)
  cmat = confusion_matrix(y_test,y_pred_class)
  tn,fp,fn,tp = cmat.ravel()
  sens = round(tp/(tp+fn),4)
  spec = round(tn/(fp+tn),4)
  yi = round(sens + spec -1,4)
  return accuracy, auc, sens, spec, yi

# Torch datasets
def to_tensor_dataset(X, y):
    return TensorDataset(torch.from_numpy(X), torch.from_numpy(y))

# Model definition: geometric-pyramid 5 layers (128,64,64,32,16)
class FFN(nn.Module):
    def __init__(self, input_dim, dropout_prob):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(dropout_prob),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout_prob),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Dropout(dropout_prob),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout_prob),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Dropout(dropout_prob),
            nn.Linear(16, 1)   # output logits
        )
    def forward(self, x):
        return self.net(x)  # logits; apply sigmoid externally if needed

# Training utilities
def train_one_epoch(model, loader, optimizer, criterion):
    model.train()
    running_loss = 0.0
    for xb, yb in loader:
        xb = xb.to(device)
        yb = yb.to(device)
        optimizer.zero_grad()
        logits = model(xb)
        loss = criterion(logits, yb)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * xb.size(0)
    return running_loss / len(loader.dataset)

@torch.no_grad()
def evaluate(model, loader, criterion=None):
    model.eval()
    losses = 0.0
    ys = []
    yps = []
    for xb, yb in loader:
        xb = xb.to(device)
        yb = yb.to(device)
        logits = model(xb)
        if criterion is not None:
            losses += criterion(logits, yb).item() * xb.size(0)
        probs = torch.sigmoid(logits).cpu().numpy().reshape(-1)
        yps.append(probs)
        ys.append(yb.cpu().numpy().reshape(-1))
    yps = np.concatenate(yps)
    ys  = np.concatenate(ys)
    avg_loss = losses / len(loader.dataset) if criterion is not None else None
    return ys, yps, avg_loss

# -----------------------------
# Reproducibility & device
# -----------------------------
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def prepare_images_and_labels(df, components, intervals, label_col, standardize=True, date_col="Date"):
    df2 = df.copy()
    # Standardize BEFORE rolling (z-score)
    if standardize:
        scaler = StandardScaler()
        df2[components] = scaler.fit_transform(df2[components].values)
        # debug prints
        print("Feature means after standardization (approx):")
        print(pd.Series(df2[components].mean()).round(6).to_dict())
        print("Feature stds after standardization (approx):")
        print(pd.Series(df2[components].std()).round(6).to_dict())

    # Build rolling means for each interval
    rolled_frames = []
    for w in intervals:
        tmp = df2[components].rolling(window=w, min_periods=1).mean()
        tmp.columns = [f"{c}_{w}" for c in components]
        rolled_frames.append(tmp)
    all_rolled = pd.concat(rolled_frames, axis=1)

    # Compose images N x 1 x 16 x 16
    dates = all_rolled.index
    N = len(dates)
    H = len(components)
    W = len(intervals)
    images = np.zeros((N, 1, H, W), dtype=np.float32)
    for i in range(N):
        row = all_rolled.iloc[i]
        for r, comp in enumerate(components):
            for c, w in enumerate(intervals):
                images[i, 0, r, c] = row[f"{comp}_{w}"]

    # mask rows with NaN in image or label
    mask_ok = (~np.isnan(images.reshape(N, -1))).all(axis=1) & (~pd.isna(df2[label_col].values))
    images = images[mask_ok]
    retained_dates = dates[mask_ok]
    labels = df2.loc[retained_dates, label_col].astype(int).values

    # Diagnostics
    print("Built images shape:", images.shape)
    unique, counts = np.unique(labels, return_counts=True)
    print("Label counts:", dict(zip(unique, counts)))
    # per-row stds
    std_map = images[:,0,:,:].reshape(images.shape[0], -1).std(axis=0).reshape(H, W)
    print("Mean std per component (rows):")
    for i, c in enumerate(components):
        print(f"{i:02d} {c:20s} mean_std={std_map[i].mean():.6f}")
    print("Mean std per interval (columns):", std_map.mean(axis=0))
    # value range
    vals = np.unique(images)
    print("Image values min/max/median:", vals.min(), vals.max(), np.median(vals), "unique_count:", len(vals))

    return images, labels, retained_dates

class HARImageDataset(Dataset):
    def __init__(self, X, y):
        self.X = X.astype(np.float32)
        self.y = y.astype(np.int64)
    def __len__(self): return len(self.y)
    def __getitem__(self, idx):
        return torch.from_numpy(self.X[idx]), int(self.y[idx])
    
class CNN_HAR_KS(nn.Module):
    def __init__(self, num_conv_layers=2, dropout_prob=0.3, num_classes=2):
        super().__init__()
        self.act = nn.ReLU()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.num_conv_layers = num_conv_layers
        if num_conv_layers == 2:
            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
            conv_out_channels = 64
        else:
            self.conv2 = None
            conv_out_channels = 32
        self.pool = nn.MaxPool2d(kernel_size=2)
        self.dropout = nn.Dropout(p=dropout_prob)
        self.fc1 = nn.Linear(conv_out_channels * 8 * 8, 64)
        self.fc_out = nn.Linear(64, num_classes)

        # weight init
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.Linear)):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)

    def forward(self, x):
        x = self.act(self.conv1(x))
        if self.conv2 is not None:
            x = self.act(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.act(self.fc1(x))
        logits = self.fc_out(x)
        return logits

def train_one_epoch_cnn(model, loader, optimizer, criterion):
    model.train()
    total_loss = 0.0
    n = 0
    for Xb, yb in loader:
        Xb = Xb.to(device).float()
        yb = yb.to(device).long()
        optimizer.zero_grad()
        logits = model(Xb)
        loss = criterion(logits, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * Xb.size(0)
        n += Xb.size(0)
    return total_loss / max(1, n)

def evaluate_cnn(model, loader, criterion):
    model.eval()
    ys, yps, losses = [], [], []
    with torch.no_grad():
        for Xb, yb in loader:
            Xb = Xb.to(device).float()
            yb = yb.to(device).long()
            logits = model(Xb)
            loss = criterion(logits, yb)
            probs = torch.softmax(logits, dim=1)[:,1].detach().cpu().numpy()
            ys.extend(yb.cpu().numpy()); yps.extend(probs)
            losses.append(loss.item() * Xb.size(0))
    if len(ys) == 0:
        return np.array([]), np.array([]), np.inf
    return np.array(ys), np.array(yps), np.sum(losses) / max(1, sum([len(batch[0]) for batch in loader]))

def train_cnn_har_ks(images, labels, components, intervals,
                     conv_layers_choices,
                     batch_sizes, dropouts,
                     lr_initial, min_lr, l2_rate,
                     patience_lr, patience_es,
                     max_epochs, test_size, val_size,
                     class_weighting, save_path, num_workers):
    # time-ordered split
    N = images.shape[0]
    idx = np.arange(N)
    train_val_idx, test_idx = train_test_split(idx, test_size=test_size, shuffle=False)
    train_idx, val_idx = train_test_split(train_val_idx, test_size=val_size/(1-test_size), shuffle=False)

    X_train, y_train = images[train_idx], labels[train_idx]
    X_val, y_val     = images[val_idx], labels[val_idx]
    X_test, y_test   = images[test_idx], labels[test_idx]

    print("Train/Val/Test sizes:", X_train.shape[0], X_val.shape[0], X_test.shape[0])

    # compute class weights (on training set)
    class_weights = None
    if class_weighting:
        unique, counts = np.unique(y_train, return_counts=True)
        freq = dict(zip(unique, counts))
        # inverse frequency weights
        total = len(y_train)
        weights = [total / (counts[i]) for i,_ in enumerate(counts)]
        # map to classes sorted ascending by unique
        class_weights = np.zeros(max(unique)+1, dtype=np.float32)
        for cls, cnt in zip(unique, counts):
            class_weights[int(cls)] = total / cnt
        print("Using class weights:", class_weights)

    best_overall = None
    best_global_val_auc = -np.inf

    for num_conv_layers in conv_layers_choices:
        for batch_size in batch_sizes:
            for dropout_prob in dropouts:
                print(f"\n=== config: conv_layers={num_conv_layers}, batch={batch_size}, dropout={dropout_prob} ===")
                model = CNN_HAR_KS(num_conv_layers=num_conv_layers, dropout_prob=dropout_prob).to(device)
                model.dropout.p = dropout_prob  # ensure dropout set

                # datasets and loaders
                train_ds = HARImageDataset(X_train, y_train)
                val_ds = HARImageDataset(X_val, y_val)
                test_ds = HARImageDataset(X_test, y_test)
                train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)
                val_loader = DataLoader(val_ds, batch_size=batch_size*2, shuffle=False, num_workers=num_workers)
                test_loader = DataLoader(test_ds, batch_size=batch_size*2, shuffle=False, num_workers=num_workers)

                # criterion (with optional class weights)
                if class_weights is not None:
                    weight_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)
                    criterion = nn.CrossEntropyLoss(weight=weight_tensor)
                else:
                    criterion = nn.CrossEntropyLoss()

                optimizer = torch.optim.Adam(model.parameters(), lr=lr_initial, weight_decay=l2_rate)
                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience_lr, min_lr=min_lr)

                best_val_auc_local = -np.inf
                best_epoch_local = 0
                best_state_local = None
                epochs_no_improve = 0

                for epoch in range(1, max_epochs+1):
                    t0 = time.time()
                    train_loss = train_one_epoch_cnn(model, train_loader, optimizer, criterion)
                    ys_val, yps_val, val_loss = evaluate_cnn(model, val_loader, criterion)
                    try:
                        val_auc = roc_auc_score(ys_val, yps_val)
                    except:
                        val_auc = 0.5

                    # scheduler monitors val_loss
                    scheduler.step(val_loss)

                    if val_auc > best_val_auc_local + 1e-12:
                        best_val_auc_local = val_auc
                        best_epoch_local = epoch
                        best_state_local = deepcopy(model.state_dict())
                        epochs_no_improve = 0
                    else:
                        epochs_no_improve += 1

                    if (epoch % 10 == 0) or epochs_no_improve == 0:
                        print(f"epoch {epoch:4d} | train_loss {train_loss:.5f} | val_loss {val_loss:.5f} | val_auc {val_auc:.4f} | best_val_auc {best_val_auc_local:.4f} | es_count {epochs_no_improve} | time {time.time()-t0:.1f}s")

                    if epochs_no_improve >= patience_es:
                        print(f"Early stopping at epoch {epoch}. Best epoch was {best_epoch_local}.")
                        break

                if best_state_local is None:
                    print("No improvement for this config -> skipping test eval.")
                    continue

                # restore best local model
                model.load_state_dict(best_state_local)

                # evaluate on test
                ys_test, yps_test, _ = evaluate(model, test_loader, criterion)
                y_pred_test = (yps_test >= 0.5).astype(int)
                test_auc = roc_auc_score(ys_test, yps_test)
                test_acc = accuracy_score(ys_test, y_pred_test)
                cm = confusion_matrix(ys_test, y_pred_test)
                tn, fp, fn, tp = cm.ravel()
                sensitivity = tp / (tp + fn + 1e-12)
                specificity = tn / (tn + fp + 1e-12)
                youden_index = sensitivity + specificity - 1

                print(f"TEST -> Accuracy: {test_acc:.4f}, AUC: {test_auc:.4f}")
                print("Confusion matrix:\n", cm)
                print(f"Best validation AUC (this config): {best_val_auc_local:.4f}")

                if best_val_auc_local > best_global_val_auc:
                    best_global_val_auc = best_val_auc_local
                    best_overall = {
                        "model_state": deepcopy(best_state_local),
                        "conv_layers": num_conv_layers,
                        "batch": batch_size,
                        "dropout": dropout_prob,
                        "best_epoch": best_epoch_local,
                        "test_accuracy": test_acc,
                        "test_auc": test_auc,
                        "sensitivity": sensitivity,
                        "specificity": specificity,
                        "youden_index": youden_index
                    }
                    torch.save(best_overall["model_state"], save_path)
                    print(f"Saved BEST model to {save_path} (val_auc={best_val_auc_local:.4f})")

    if best_overall is None:
        raise RuntimeError("No model trained successfully.")
    print("\n================ BEST MODEL SUMMARY ================")
    print(f"Best configuration: conv_layers={best_overall['conv_layers']}, batch={best_overall['batch']}, dropout={best_overall['dropout']}")
    print(f"Best epoch (val-AUC): {best_overall['best_epoch']}")
    print(f"Test Accuracy: {best_overall['test_accuracy']:.4f}  Test AUC: {best_overall['test_auc']:.4f}")
    print(f"Sensitivity: {best_overall['sensitivity']:.4f}  Specificity: {best_overall['specificity']:.4f}  Youden: {best_overall['youden_index']:.4f}")

    return best_overall

# Main loop over tickers
tickers = ['AXP','BA','CAT','CSCO','CVX','AAPL','GE','GS','HD','IBM','INTC','JNJ','JPM','KO','MCD','MMM','MRK','MSFT','NKE','PFE','PG','VZ','WMT','XOM']
HAR_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
HARJ_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
HARCJ_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
HARRSI_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
HARRSII_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
HARRSJI_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
HARRSJII_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
HAR_KS_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
HAR_myProposal_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
RF_HAR_KS_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
RF_HAR_myProposal_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
LGBM_HAR_KS_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
LGBM_HAR_myProposal_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
XGB_HAR_KS_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
XGB_HAR_myProposal_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
FFN_HAR_KS_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}
GCNN_HAR_KS_scores = {'Accuracy':0,'AUC':0,'Sensitivity':0,'Specificity':0,'Youden_Index':0}

for ticker in tqdm(tickers):
    print("Processing ticker:", ticker)
    df = pd.DataFrame()
    filepath = os.path.join(base, "RealLogReturns", f"{ticker}.csv")
    while(len(df)!=252021):
        df = pd.read_csv(filepath)

    df.rename(columns={ticker: "log_return"}, inplace=True)
    df["Date"] = pd.to_datetime(df["Date"], format="%Y%m%d")
    df['log_return_sq'] = df['log_return']**2

    rv_df = df.groupby('Date', as_index=False)['log_return_sq'].sum()
    rv_df = rv_df.rename(columns={'log_return_sq': 'RV'})
    rv_df['RVD'] = rv_df['RV'].shift(1)
    rv_df['RVW'] = rv_df['RV'].rolling(window=5).mean().shift(1)
    rv_df['RVM'] = rv_df['RV'].rolling(window=22).mean().shift(1)
    rv_df['RV_dir'] = np.where(rv_df['RV'] - rv_df['RV'].shift(1) < 0, 1, 0)

    # Apply function
    bpv_df = compute_bpv(df)
    merged_df1 = pd.merge(rv_df, bpv_df, on="Date", how="left")

    merged_df1["Jump"] = np.maximum(merged_df1["RV"] - merged_df1["BPV"], 0)
    merged_df1["CSP"] = merged_df1["RV"] - merged_df1["Jump"]
    merged_df1["CJ"] = merged_df1["Jump"]

    # Daily, Weekly (5 trading days), Monthly (~22 trading days) rolling averages
    merged_df1["CSPw"] = merged_df1["CSP"].rolling(5).mean()
    merged_df1["CSPm"] = merged_df1["CSP"].rolling(22).mean()
    merged_df1["CJw"]  = merged_df1["CJ"].rolling(5).mean()
    merged_df1["CJm"]  = merged_df1["CJ"].rolling(22).mean()
    merged_df1.drop(["Jump"], axis=1, inplace=True)

    shar_df1 = df.groupby("Date")["log_return"].agg(
        RS_plus=rv_plus,
        RS_minus=rv_minus
    ).reset_index()

    merged_df2 = pd.merge(merged_df1, shar_df1, on="Date", how="left")
    # Step 1: Create negative indicator per day (returns True/False for each Date)
    neg_indicator = df.groupby("Date")["log_return"].apply(lambda x: (x < 0).any()).astype(int)
    merged_df2 = merged_df2.merge(neg_indicator.rename("neg_indicator"), on="Date", how="left")

    # Step 3: Compute RV_neg_indicator
    merged_df2["RV_neg_indicator"] = merged_df2["RV"] * merged_df2["neg_indicator"]
    # 1️⃣ Signed Jump Variation (SJ)
    merged_df2["SJ"] = merged_df2["RS_plus"] - merged_df2["RS_minus"]

    # 2️⃣ Split into positive and negative parts for HARRSJII
    merged_df2["SJ_plus"]  = merged_df2["SJ"].where(merged_df2["SJ"] > 0, 0)
    merged_df2["SJ_minus"] = merged_df2["SJ"].where(merged_df2["SJ"] < 0, 0)

    # 3️⃣ Realized Tri-Power Quarticity (TQ)
    # μ_{4/3} = E(|Z|^{4/3}) for Z ~ N(0,1)
    mu_4by3 = np.mean(np.abs(np.random.normal(size=10_000_000)) ** (4/3))
    tq_df = df.groupby("Date").apply(realized_tri_power_quarticity).reset_index(name="TQ")
    harq_df1 = (
        df
        .groupby("Date")["log_return"]
        .apply(lambda x: (78/3) * (x**4).sum())
        .reset_index(name="RQ")
    )

    merged_df3 = merged_df2.merge(tq_df, on="Date", how="left")
    merged_df3.drop(["neg_indicator"], axis=1, inplace=True)

    merged_df4 = merged_df3.merge(harq_df1, on="Date", how="left")

    M = 78  # or your number of intraday returns
    merged_df4["ABD_Z"] = (merged_df4["RV"] - merged_df4["BPV"]) / np.sqrt((merged_df4["TQ"] / (merged_df4["BPV"]**2)) / M)
    merged_df4["ABD_I"] = (merged_df4["ABD_Z"] > 1.96).astype(int)
    merged_df4["Jump_ABD"] = merged_df4["ABD_I"] * (merged_df4["RV"] - merged_df4["BPV"])
    merged_df4["CSP_ABD"] = merged_df4["RV"] - merged_df4["Jump_ABD"]

    merged_df4["BNS_Z"] = (merged_df4["RV"] - merged_df4["BPV"]) / np.sqrt((np.pi**2 / 4) * (merged_df4["TQ"] / M))
    merged_df4["BNS_I"] = (merged_df4["BNS_Z"] > 1.96).astype(int)
    merged_df4["Jump_BNS"] = merged_df4["BNS_I"] * (merged_df4["RV"] - merged_df4["BPV"])
    merged_df4["CSP_BNS"] = merged_df4["RV"] - merged_df4["Jump_BNS"]

    daily_log_return = df.groupby("Date")["log_return"].sum()
    daily_log_return = daily_log_return.rename("L_daily")
    daily_simple_return = np.exp(daily_log_return) - 1
    daily_simple_return = daily_simple_return.rename("R_daily")

    merged_df5 = merged_df4.merge(daily_log_return, left_on="Date", right_index=True)
    merged_df6 = merged_df5.merge(daily_simple_return, left_on="Date", right_index=True)

    merged_df6["D_JO"] = merged_df6["L_daily"] - (
        merged_df6["R_daily"] - 0.5 * merged_df6["RV"]
    )
    merged_df6["JO_Z"] = merged_df6["D_JO"] / np.sqrt(merged_df6["TQ"] / 4)
    merged_df6["JO_I"] = (merged_df6["JO_Z"].abs() > 1.96).astype(int)
    merged_df6["Jump_JO"] = merged_df6["JO_I"] * (merged_df6["RV"] - merged_df6["BPV"])
    merged_df6["CSP_JO"] = merged_df6["RV"] - merged_df6["Jump_JO"]

    merged_df7 = merged_df6.drop(['ABD_Z','ABD_I','BNS_Z','BNS_I','R_daily','D_JO','JO_Z','JO_I'],axis=1)

    cols_to_shift = [
        "BPV", "CSP", "CJ", "RS_plus", "RS_minus",
        "RV_neg_indicator", "SJ", "SJ_plus", "SJ_minus", "TQ","RQ", 'Jump_ABD', 'CSP_ABD', 'Jump_BNS',
        'CSP_BNS', 'L_daily', 'Jump_JO', 'CSP_JO'
    ]

    merged_df7[cols_to_shift] = merged_df7[cols_to_shift].shift(1)
    main_df = merged_df7.dropna()
    split_index = int(0.75 * len(main_df))

    train_df = main_df.iloc[:split_index]
    test_df = main_df.iloc[split_index:]

    """# **Standard HAR**"""
    try:
        X_train, y_train, X_test, y_test = data(['RVD','RVW','RVM'])
        HAR = sm.Logit(y_train, X_train)
        HAR_results = HAR.fit(method='bfgs', maxiter=100, disp=True)
        y_pred = HAR_results.predict(X_test)
        accuracy, auc, sens, spec, yi = perf_eval(y_pred, y_test)
        HAR_scores['Accuracy']+=accuracy
        HAR_scores['AUC']+=auc
        HAR_scores['Sensitivity']+=sens
        HAR_scores['Specificity']+=spec
        HAR_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in HAR model for ticker", ticker, ":", e)

    """# **logistic-HARJ**"""
    try:
        X_train, y_train, X_test, y_test = data(['RVD','RVW','RVM','CJ'])
        HARJ = sm.Logit(y_train, X_train)
        HARJ_results = HARJ.fit(method='bfgs', maxiter=100, disp=True)
        y_pred = HARJ_results.predict(X_test)
        accuracy, auc, sens, spec, yi = perf_eval(y_pred, y_test)
        HARJ_scores['Accuracy']+=accuracy
        HARJ_scores['AUC']+=auc
        HARJ_scores['Sensitivity']+=sens
        HARJ_scores['Specificity']+=spec
        HARJ_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in HARJ model for ticker", ticker, ":", e)

    """# **logistic-HARCJ**"""
    try:
        X_train, y_train, X_test, y_test = data(['CSP','CSPw','CSPm','CJ','CJw','CJm'])
        HARCJ = sm.Logit(y_train, X_train)
        HARCJ_results = HARCJ.fit(method='bfgs', maxiter=100, disp=True)
        y_pred = HARCJ_results.predict(X_test)
        accuracy, auc, sens, spec, yi = perf_eval(y_pred, y_test)
        HARCJ_scores['Accuracy']+=accuracy
        HARCJ_scores['AUC']+=auc
        HARCJ_scores['Sensitivity']+=sens
        HARCJ_scores['Specificity']+=spec
        HARCJ_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in HARCJ model for ticker", ticker, ":", e)

    """# **logistic-HARRSI**"""
    try:
        X_train, y_train, X_test, y_test = data(['RS_plus','RS_minus','RVW','RVM'])
        HARRSI = sm.Logit(
            y_train, X_train
        )
        HARRSI_results = HARRSI.fit(method='bfgs', maxiter=100, disp=True)
        y_pred = HARRSI_results.predict(X_test)
        accuracy, auc, sens, spec, yi = perf_eval(y_pred, y_test)
        HARRSI_scores['Accuracy']+=accuracy
        HARRSI_scores['AUC']+=auc
        HARRSI_scores['Sensitivity']+=sens
        HARRSI_scores['Specificity']+=spec
        HARRSI_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in HARRSI model for ticker", ticker, ":", e)

    """# **logistic-HARRSII**"""
    try:
        X_train, y_train, X_test, y_test = data(['RS_plus','RS_minus','RV_neg_indicator','RVW','RVM'])
        HARRSII = sm.Logit(
            y_train, X_train
        )
        HARRSII_results = HARRSII.fit(method='bfgs', maxiter=100, disp=True)
        y_pred = HARRSII_results.predict(X_test)
        accuracy, auc, sens, spec, yi = perf_eval(y_pred, y_test)
        HARRSII_scores['Accuracy']+=accuracy
        HARRSII_scores['AUC']+=auc
        HARRSII_scores['Sensitivity']+=sens
        HARRSII_scores['Specificity']+=spec
        HARRSII_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in HARRSII model for ticker", ticker, ":", e)

    """# **logistic-HARRSJI**"""
    try:
        X_train, y_train, X_test, y_test = data(['SJ','BPV','RVW','RVM'])
        HARRSJI = sm.Logit(
            y_train, X_train
        )
        HARRSJI_results = HARRSJI.fit(method='bfgs', maxiter=100, disp=True)
        y_pred = HARRSJI_results.predict(X_test)
        accuracy, auc, sens, spec, yi = perf_eval(y_pred, y_test)
        HARRSJI_scores['Accuracy']+=accuracy
        HARRSJI_scores['AUC']+=auc
        HARRSJI_scores['Sensitivity']+=sens
        HARRSJI_scores['Specificity']+=spec
        HARRSJI_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in HARRSJI model for ticker", ticker, ":", e)

    """# **logistic-HARRSJII**"""
    try:
        X_train, y_train, X_test, y_test = data(['SJ_plus','SJ_minus','BPV','RVW','RVM'])
        HARRSJII = sm.Logit(
            y_train, X_train
        )
        HARRSJII_results = HARRSJII.fit(method='bfgs', maxiter=100, disp=True)
        y_pred = HARRSJII_results.predict(X_test)
        accuracy, auc, sens, spec, yi = perf_eval(y_pred, y_test)
        HARRSJII_scores['Accuracy']+=accuracy
        HARRSJII_scores['AUC']+=auc
        HARRSJII_scores['Sensitivity']+=sens
        HARRSJII_scores['Specificity']+=spec
        HARRSJII_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in HARRSJII model for ticker", ticker, ":", e)

    """# **logistic-HAR-KS**"""
    try:
        X_train, y_train, X_test, y_test = data(['RVD','BPV','Jump_ABD','CSP_ABD','Jump_BNS','CSP_BNS','Jump_JO','CSP_JO','RS_plus','RS_minus','L_daily','RV_neg_indicator','SJ', 'SJ_plus', 'SJ_minus', 'TQ'])
        HAR_KS = sm.Logit(
            y_train, X_train
        )
        HAR_KS_results = HAR_KS.fit(method='bfgs',maxiter=100, disp=True)
        y_pred = HAR_KS_results.predict(X_test)
        accuracy, auc, sens, spec, yi = perf_eval(y_pred, y_test)
        HAR_KS_scores['Accuracy']+=accuracy
        HAR_KS_scores['AUC']+=auc
        HAR_KS_scores['Sensitivity']+=sens
        HAR_KS_scores['Specificity']+=spec
        HAR_KS_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in HAR-KS model for ticker", ticker, ":", e)

    """# **logistic-HAR-myProposal**"""
    try:
        X_train, y_train, X_test, y_test = data(['CSP','CSPw','CJ','CJw','SJ','BPV','RQ'])
        HAR_KS = sm.Logit(
            y_train, X_train
        )
        HAR_KS_results = HAR_KS.fit(method='bfgs',maxiter=100, disp=True)
        y_pred = HAR_KS_results.predict(X_test)
        accuracy, auc, sens, spec, yi = perf_eval(y_pred, y_test)
        HAR_myProposal_scores['Accuracy']+=accuracy
        HAR_myProposal_scores['AUC']+=auc
        HAR_myProposal_scores['Sensitivity']+=sens
        HAR_myProposal_scores['Specificity']+=spec
        HAR_myProposal_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in HAR-myProposal model for ticker", ticker, ":", e)

    """# **RF-HAR-KS**"""
    try:
        X_train, y_train, X_test, y_test = data(['RVD','BPV','Jump_ABD','CSP_ABD','Jump_BNS','CSP_BNS','Jump_JO','CSP_JO','RS_plus','RS_minus','L_daily','RV_neg_indicator','SJ', 'SJ_plus', 'SJ_minus', 'TQ'])
        p = X_train.shape[1]
        ntry = max(1, int(round(p / 3)))
        rf = RandomForestClassifier(
            n_estimators=500,
            max_features=ntry,
            random_state=42,
            n_jobs=-1,
            class_weight=None  # or 'balanced' if classes are imbalanced
        )
        rf.fit(X_train, y_train)
        y_prob = rf.predict_proba(X_test)[:, 1]   # probability of class 1
        y_pred = (y_prob >= 0.5).astype(int)
        acc = round(accuracy_score(y_test, y_pred),4)
        auc = round(roc_auc_score(y_test, y_prob),4)
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
        sens = round(tp/(tp+fn),4)
        spec = round(tn/(fp+tn),4)
        yi = round(sens + spec -1,4)
        RF_HAR_KS_scores['Accuracy']+=acc
        RF_HAR_KS_scores['AUC']+=auc
        RF_HAR_KS_scores['Sensitivity']+=sens
        RF_HAR_KS_scores['Specificity']+=spec
        RF_HAR_KS_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in RF-HAR-KS model for ticker", ticker, ":", e)

    """# **RF-HAR-MyProposal**"""
    try:
        X_train, y_train, X_test, y_test = data(['CSP','CSPw','CJ','CJw','SJ','BPV','RQ'])
        p = X_train.shape[1]
        ntry = max(1, int(round(p / 3)))
        rf = RandomForestClassifier(
            n_estimators=500,
            max_features=ntry,
            random_state=42,
            n_jobs=-1,
            class_weight=None  # or 'balanced' if classes are imbalanced
        )
        rf.fit(X_train, y_train)
        y_prob = rf.predict_proba(X_test)[:, 1]   # probability of class 1
        y_pred = (y_prob >= 0.5).astype(int)
        acc = round(accuracy_score(y_test, y_pred),4)
        auc = round(roc_auc_score(y_test, y_prob),4)
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
        sens = round(tp/(tp+fn),4)
        spec = round(tn/(fp+tn),4)
        yi = round(sens + spec -1,4)
        RF_HAR_myProposal_scores['Accuracy']+=acc
        RF_HAR_myProposal_scores['AUC']+=auc
        RF_HAR_myProposal_scores['Sensitivity']+=sens
        RF_HAR_myProposal_scores['Specificity']+=spec
        RF_HAR_myProposal_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in RF-HAR-MyProposal model for ticker", ticker, ":", e)

    """# **LightGBM-HAR-KS**"""
    try:
        X_train, y_train, X_test, y_test = data(['RVD','BPV','Jump_ABD','CSP_ABD','Jump_BNS','CSP_BNS','Jump_JO','CSP_JO','RS_plus','RS_minus','L_daily','RV_neg_indicator','SJ', 'SJ_plus', 'SJ_minus', 'TQ'])
        # --- 3. LightGBM dataset objects ---
        dtrain = lgb.Dataset(X_train, label=y_train, free_raw_data=False)
        dvalid = lgb.Dataset(X_test, label=y_test, reference=dtrain, free_raw_data=False)
        # --- 4. Parameters (aligned with Appendix A recommendations / common defaults) ---
        params = {
            'objective': 'binary',           # binary classification (direction)
            'boosting_type': 'gbdt',         # standard LightGBM GBDT (leaf-wise)
            'metric': 'auc',                 # optimize AUC (paper uses AUC as key metric)
            'num_leaves': 31,                # complexity of each tree (paper suggests tuning; 31 is common)
            'learning_rate': 0.05,           # shrinkage (Appendix suggests small lr for stability)
            'feature_fraction': 0.8,         # EFB/feature subsampling
            'bagging_fraction': 0.8,         # row subsampling (GOSS & bagging interplay)
            'bagging_freq': 1,               # perform bagging every iteration
            'lambda_l2': 0.1,                # L2 regularization on leaf weights (lambda in appendix)
            'min_data_in_leaf': 20,          # min samples per leaf (stability)
            'max_depth': -1,                 # no explicit depth limit (leaf-wise controls shape)
            'verbosity': -1,
            'seed': 42,
            'nthread': -1
        }
        # training rounds and early stopping
        num_boost_round = 1000
        callbacks = [
            early_stopping(stopping_rounds=50),
            log_evaluation(period=25)
        ]
        bst = lgb.train(
            params=params,
            train_set=dtrain,
            num_boost_round=num_boost_round,
            valid_sets=[dvalid],
            valid_names=['valid'],
            callbacks=callbacks
        )
        # --- 6. Predict & evaluate ---
        y_prob = bst.predict(X_test, num_iteration=bst.best_iteration)  # probabilities in [0,1]
        y_pred = (y_prob >= 0.5).astype(int)
        acc = round(accuracy_score(y_test, y_pred),4)
        auc = round(roc_auc_score(y_test, y_prob),4)
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
        sens = round(tp/(tp+fn),4)
        spec = round(tn/(fp+tn),4)
        yi = round(sens + spec -1,4)
        LGBM_HAR_KS_scores['Accuracy']+=acc
        LGBM_HAR_KS_scores['AUC']+=auc
        LGBM_HAR_KS_scores['Sensitivity']+=sens
        LGBM_HAR_KS_scores['Specificity']+=spec
        LGBM_HAR_KS_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in LGBM-HAR-KS model for ticker", ticker, ":", e)

    """# **LightGBM-HAR-MyProposal**"""
    try:
        X_train, y_train, X_test, y_test = data(['CSP','CSPw','CJ','CJw','SJ','BPV','RQ'])
        # --- 3. LightGBM dataset objects ---
        dtrain = lgb.Dataset(X_train, label=y_train, free_raw_data=False)
        dvalid = lgb.Dataset(X_test, label=y_test, reference=dtrain, free_raw_data=False)
        # --- 4. Parameters (aligned with Appendix A recommendations / common defaults) ---
        params = {
            'objective': 'binary',           # binary classification (direction)
            'boosting_type': 'gbdt',         # standard LightGBM GBDT (leaf-wise)
            'metric': 'auc',                 # optimize AUC (paper uses AUC as key metric)
            'num_leaves': 31,                # complexity of each tree (paper suggests tuning; 31 is common)
            'learning_rate': 0.05,           # shrinkage (Appendix suggests small lr for stability)
            'feature_fraction': 0.8,         # EFB/feature subsampling
            'bagging_fraction': 0.8,         # row subsampling (GOSS & bagging interplay)
            'bagging_freq': 1,               # perform bagging every iteration
            'lambda_l2': 0.1,                # L2 regularization on leaf weights (lambda in appendix)
            'min_data_in_leaf': 20,          # min samples per leaf (stability)
            'max_depth': -1,                 # no explicit depth limit (leaf-wise controls shape)
            'verbosity': -1,
            'seed': 42,
            'nthread': -1
        }
        # training rounds and early stopping
        num_boost_round = 1000
        callbacks = [
            early_stopping(stopping_rounds=50),
            log_evaluation(period=25)
        ]
        bst = lgb.train(
            params=params,
            train_set=dtrain,
            num_boost_round=num_boost_round,
            valid_sets=[dvalid],
            valid_names=['valid'],
            callbacks=callbacks
        )
        # --- 6. Predict & evaluate ---
        y_prob = bst.predict(X_test, num_iteration=bst.best_iteration)  # probabilities in [0,1]
        y_pred = (y_prob >= 0.5).astype(int)
        acc = round(accuracy_score(y_test, y_pred),4)
        auc = round(roc_auc_score(y_test, y_prob),4)
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
        sens = round(tp/(tp+fn),4)
        spec = round(tn/(fp+tn),4)
        yi = round(sens + spec -1,4)
        LGBM_HAR_myProposal_scores['Accuracy']+=acc
        LGBM_HAR_myProposal_scores['AUC']+=auc
        LGBM_HAR_myProposal_scores['Sensitivity']+=sens
        LGBM_HAR_myProposal_scores['Specificity']+=spec
        LGBM_HAR_myProposal_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in LGBM-HAR-MyProposal model for ticker", ticker, ":", e)

    """# **XGB-HAR-KS**"""
    try:
        X_train, y_train, X_test, y_test = data(['RVD','BPV','Jump_ABD','CSP_ABD','Jump_BNS','CSP_BNS','Jump_JO','CSP_JO','RS_plus','RS_minus','L_daily','RV_neg_indicator','SJ', 'SJ_plus', 'SJ_minus', 'TQ'])
        split_index_val = int(0.4 * len(X_test))
        X_val = X_test.iloc[:split_index_val]
        X_test = X_test.iloc[split_index_val:]
        y_val = y_test.iloc[:split_index_val]
        y_test = y_test.iloc[split_index_val:]
        # --- 1. Prepare DMatrix ---
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)
        dtest = xgb.DMatrix(X_test, label=y_test)
        # --- 2. Parameters (aligned with Appendix B and paper) ---
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'eta': 0.07,                 # learning rate
            'max_depth': 4,              # tree depth
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'lambda': 0.5,               # L2 regularization term
            'gamma': 0.05,                # leaf penalty term
            'min_child_weight': 1,
            'scale_pos_weight': 1,       # assume balanced data
            'verbosity': 1,
            'seed': 42
        }
        # --- 3. Train model with validation ---
        evals = [(dtrain, 'train'), (dval, 'val')]
        num_boost_round = 1000
        bst = xgb.train(
            params,
            dtrain,
            num_boost_round=num_boost_round,
            evals=evals,
            early_stopping_rounds=50,
            verbose_eval=50
        )
        # --- 4. Evaluate on test set ---
        y_pred_prob = bst.predict(dtest)
        y_pred = (y_pred_prob > 0.5).astype(int)
        acc = round(accuracy_score(y_test, y_pred),4)
        auc = round(roc_auc_score(y_test, y_pred),4)
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
        sens = round(tp/(tp+fn),4)
        spec = round(tn/(fp+tn),4)
        yi = round(sens + spec -1,4)
        XGB_HAR_KS_scores['Accuracy']+=acc
        XGB_HAR_KS_scores['AUC']+=auc
        XGB_HAR_KS_scores['Sensitivity']+=sens
        XGB_HAR_KS_scores['Specificity']+=spec
        XGB_HAR_KS_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in XGB-HAR-KS model for ticker", ticker, ":", e)

    """# **XGB-HAR-MyProposal**"""
    try:
        X_train, y_train, X_test, y_test = data(['CSP','CSPw','CJ','CJw','SJ','BPV','RQ'])
        split_index_val = int(0.4 * len(X_test))
        X_val = X_test.iloc[:split_index_val]
        X_test = X_test.iloc[split_index_val:]
        y_val = y_test.iloc[:split_index_val]
        y_test = y_test.iloc[split_index_val:]
        # --- 1. Prepare DMatrix ---
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)
        dtest = xgb.DMatrix(X_test, label=y_test)
        # --- 2. Parameters (aligned with Appendix B and paper) ---
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'eta': 0.07,                 # learning rate
            'max_depth': 4,              # tree depth
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'lambda': 0.5,               # L2 regularization term
            'gamma': 0.05,                # leaf penalty term
            'min_child_weight': 1,
            'scale_pos_weight': 1,       # assume balanced data
            'verbosity': 1,
            'seed': 42
        }
        # --- 3. Train model with validation ---
        evals = [(dtrain, 'train'), (dval, 'val')]
        num_boost_round = 1000
        bst = xgb.train(
            params,
            dtrain,
            num_boost_round=num_boost_round,
            evals=evals,
            early_stopping_rounds=50,
            verbose_eval=50
        )
        # --- 4. Evaluate on test set ---
        y_pred_prob = bst.predict(dtest)
        y_pred = (y_pred_prob > 0.5).astype(int)
        acc = round(accuracy_score(y_test, y_pred),4)
        auc = round(roc_auc_score(y_test, y_pred),4)
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
        sens = round(tp/(tp+fn),4)
        spec = round(tn/(fp+tn),4)
        yi = round(sens + spec -1,4)
        XGB_HAR_myProposal_scores['Accuracy']+=acc
        XGB_HAR_myProposal_scores['AUC']+=auc
        XGB_HAR_myProposal_scores['Sensitivity']+=sens
        XGB_HAR_myProposal_scores['Specificity']+=spec
        XGB_HAR_myProposal_scores['Youden_Index']+=yi
    except Exception as e:
        print("Error in XGB-HAR-MyProposal model for ticker", ticker, ":", e)

    """# **FFN-HAR-KS**"""
    try:
        X_train, y_train, X_test, y_test = data(['RVD','BPV','Jump_ABD','CSP_ABD','Jump_BNS','CSP_BNS','Jump_JO','CSP_JO','RS_plus','RS_minus','L_daily','RV_neg_indicator','SJ', 'SJ_plus', 'SJ_minus', 'TQ'])
        # Convert pandas -> numpy
        X_train_np = X_train.values.astype(np.float32)
        X_test_np  = X_test.values.astype(np.float32)
        y_train_np = y_train.values.astype(np.float32).reshape(-1, 1)
        y_test_np  = y_test.values.astype(np.float32).reshape(-1, 1)
        # Standardize features (fit on train only)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_np)
        X_test_scaled = scaler.transform(X_test_np)
        # Save scaler for inference later
        joblib.dump(scaler, "ffn_scaler.pkl")
        # Create validation split from training set (last 20% of the training part)
        val_frac = 0.2
        n_train = X_train_scaled.shape[0]
        n_val = int(math.ceil(n_train * val_frac))
        n_train_sub = n_train - n_val

        X_train_sub = X_train_scaled[:n_train_sub]
        y_train_sub = y_train_np[:n_train_sub]
        X_val = X_train_scaled[n_train_sub:]
        y_val = y_train_np[n_train_sub:]

        train_ds = to_tensor_dataset(X_train_sub, y_train_sub)
        val_ds   = to_tensor_dataset(X_val, y_val)
        test_ds  = to_tensor_dataset(X_test_scaled, y_test_np)
        
        # Hyperparameter grid from paper
        batch_sizes = [8, 16]
        dropouts = [0.3, 0.7]
        l2_rate = 0.001   # weight decay
        lr_initial = 1e-3 # we'll use ReduceLROnPlateau; starting LR chosen sensibly
        min_lr = 1e-4
        max_epochs = 3000
        patience_es = 100     # early stopping patience (this is long; paper used early stopping)
        patience_lr = 20      # patience for ReduceLROnPlateau
        verbose = True

        best_overall = None
        best_global_auc = -np.inf

        for batch_size in batch_sizes:
            for dropout_prob in dropouts:
                print(f"\n--- Training FFN: batch={batch_size}, dropout={dropout_prob} ---")
                model = FFN(input_dim=X_train_sub.shape[1], dropout_prob=dropout_prob).to(device)
                criterion = nn.BCEWithLogitsLoss()
                optimizer = torch.optim.Adam(model.parameters(), lr=lr_initial, weight_decay=l2_rate)
                train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
                val_loader   = DataLoader(val_ds,   batch_size=batch_size*2, shuffle=False)
                test_loader  = DataLoader(test_ds,  batch_size=batch_size*2, shuffle=False)
                # ---------- FIXED: AUC-based tracking ----------
                best_auc_local = -np.inf
                best_epoch_local = 0
                best_state_local = None
                epochs_no_improve = 0
                for epoch in range(1, max_epochs + 1):
                    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)
                    ys_val, yps_val, val_loss = evaluate(model, val_loader, criterion)
                    try:
                        val_auc = roc_auc_score(ys_val, yps_val)
                    except:
                        val_auc = 0.5
                    # ---------- FIX: EARLY STOPPING & BEST MODEL BOTH ON AUC ----------
                    if val_auc > best_auc_local + 1e-12:
                        best_auc_local = val_auc
                        best_epoch_local = epoch
                        best_state_local = deepcopy(model.state_dict())
                        epochs_no_improve = 0
                    else:
                        epochs_no_improve += 1
                    if epochs_no_improve >= patience_es:
                        print(f"Early stopping at epoch {epoch}. Best epoch was {best_epoch_local}.")
                        break
                # Load best model state for this config
                model.load_state_dict(best_state_local)
                # -------------------- Test evaluation --------------------
                ys_test, yps_test, _ = evaluate(model, test_loader, criterion)
                y_pred_test = (yps_test >= 0.5).astype(int)
                test_auc = roc_auc_score(ys_test, yps_test)
                test_acc = accuracy_score(ys_test, y_pred_test)
                cm = confusion_matrix(ys_test, y_pred_test)
                print(f"TEST -> Accuracy: {test_acc:.4f}, AUC: {test_auc:.4f}")
                print("Confusion matrix:\n", cm)
                # sensitivity, specificity, Youden
                tn, fp, fn, tp = cm.ravel()
                sensitivity = tp / (tp + fn + 1e-12)
                specificity = tn / (tn + fp + 1e-12)
                youden = sensitivity + specificity - 1
                # ---------- FIX: GLOBAL MODEL SELECTOR uses AUC ----------
                if best_auc_local > best_global_auc:
                    best_global_auc = best_auc_local
                    best_overall = {
                        "model_state": deepcopy(best_state_local),
                        "batch": batch_size,
                        "dropout": dropout_prob,
                        "best_epoch": best_epoch_local,
                        "test_accuracy": test_acc,
                        "test_auc": test_auc,
                        "sensitivity": sensitivity,
                        "specificity": specificity,
                        "youden_index": youden
                    }
        
        FFN_HAR_KS_scores['Accuracy']+=round(best_overall['test_accuracy'],4)
        FFN_HAR_KS_scores['AUC']+=round(best_overall['test_auc'],4)
        FFN_HAR_KS_scores['Sensitivity']+=round(best_overall['sensitivity'],4)
        FFN_HAR_KS_scores['Specificity']+=round(best_overall['specificity'],4)
        FFN_HAR_KS_scores['Youden_Index']+=round(best_overall['youden_index'],4)
    except Exception as e:
        print("Error in FFN-HAR-KS model for ticker", ticker, ":", e)

    """# **GCNN-HAR-KS**"""
    try:
        label_col = "RV_dir"   # target (0/1)
        components = [
            'RV','BPV','Jump_ABD','CSP_ABD',
            'Jump_BNS','CSP_BNS','Jump_JO','CSP_JO',
            'RS_plus','RS_minus','L_daily','RV_neg_indicator',
            'SJ','SJ_plus','SJ_minus','TQ'
        ]
        intervals = [1] + list(range(6, 21))  # [1,6,7,...,20] -> length 16
        # Training / grid search settings (Appendix C)
        BATCH_SIZES = [8, 16]
        DROPOUTS = [0.3, 0.7]
        L2_RATE = 0.001
        LR_INITIAL = 1e-3
        MIN_LR = 1e-4
        PATIENCE_LR = 20
        MAX_EPOCHS = 3000
        PATIENCE_ES = 50   # early-stopping patience (tune down to 50 for quick debug)
        NUM_CONV_LAYERS_CHOICES = [1, 2]  # paper had [1,2]
        TEST_SIZE = 0.2
        VAL_SIZE = 0.2
        STANDARDIZE_BEFORE_ROLL = True
        CLASS_WEIGHTING = True  # set True to use class weights in CrossEntropyLoss if imbalance exists
        SEED = 42
        NUM_WORKERS = 4
        SAVE_PATH = "cnn_har_ks_best.pt"
        set_seed(SEED)
        images, labels, retained_dates = prepare_images_and_labels(main_df, components, intervals, label_col, standardize=STANDARDIZE_BEFORE_ROLL)
        # optional quick sanity overfit test (small)
        # (user may skip, but recommended)
        # train full model grid
        best = train_cnn_har_ks(images, labels, components, intervals,
                                conv_layers_choices=NUM_CONV_LAYERS_CHOICES,
                                batch_sizes=BATCH_SIZES, dropouts=DROPOUTS,
                                lr_initial=LR_INITIAL, min_lr=MIN_LR, l2_rate=L2_RATE,
                                patience_lr=PATIENCE_LR, patience_es=PATIENCE_ES,
                                max_epochs=MAX_EPOCHS, test_size=TEST_SIZE, val_size=VAL_SIZE,
                                class_weighting=CLASS_WEIGHTING, save_path=SAVE_PATH, num_workers=NUM_WORKERS)
        print("Best result:", best['test_accuracy'])
        GCNN_HAR_KS_scores['Accuracy']+=round(best['test_accuracy'],4)
        GCNN_HAR_KS_scores['AUC']+=round(best['test_auc'],4)
        GCNN_HAR_KS_scores['Sensitivity']+=round(best['sensitivity'],4)
        GCNN_HAR_KS_scores['Specificity']+=round(best['specificity'],4)
        GCNN_HAR_KS_scores['Youden_Index']+=round(best['youden_index'],4)
    except Exception as e:
        print("Error in GCNN-HAR-KS model for ticker", ticker, ":", e)

    print("="*10+"Completed processing for ticker:", ticker +"="*10)

# Average results over all tickers
num_tickers = len(tickers)
for result_dict in [HAR_scores, HARJ_scores, HARCJ_scores, HARRSI_scores, HARRSII_scores,
                    HARRSJI_scores, HARRSJII_scores, HAR_KS_scores, HAR_myProposal_scores,
                    RF_HAR_KS_scores, RF_HAR_myProposal_scores,
                    LGBM_HAR_KS_scores, LGBM_HAR_myProposal_scores,
                    XGB_HAR_KS_scores, XGB_HAR_myProposal_scores, FFN_HAR_KS_scores, GCNN_HAR_KS_scores]:
    for key in result_dict:
        result_dict[key] /= num_tickers

# Display final averaged results
print("Final Averaged Results over all tickers:")
print("HAR Results:", HAR_scores)
print("HARJ Results:", HARJ_scores)
print("HARCJ Results:", HARCJ_scores)
print("HARRSI Results:", HARRSI_scores)
print("HARRSII Results:", HARRSII_scores)
print("HARRSJI Results:", HARRSJI_scores)
print("HARRSJII Results:", HARRSJII_scores)
print("HAR-KS Results:", HAR_KS_scores)
print("HAR-myProposal Results:", HAR_myProposal_scores)
print("RF-HAR-KS Results:", RF_HAR_KS_scores)
print("RF-HAR-myProposal Results:", RF_HAR_myProposal_scores)
print("LGBM-HAR-KS Results:", LGBM_HAR_KS_scores)
print("LGBM-HAR-myProposal Results:", LGBM_HAR_myProposal_scores)
print("XGB-HAR-KS Results:", XGB_HAR_KS_scores)
print("XGB-HAR-myProposal Results:", XGB_HAR_myProposal_scores)
print("FFN-HAR-KS Results:", FFN_HAR_KS_scores)
print("GCNN-HAR-KS Results:", GCNN_HAR_KS_scores)